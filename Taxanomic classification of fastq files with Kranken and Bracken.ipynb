{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12310273,"sourceType":"datasetVersion","datasetId":7759369},{"sourceId":13158555,"sourceType":"datasetVersion","datasetId":8337523}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================= Kaggle Setup for vsearch + Kraken2 =================\nimport os\n\n# Update apt repo\n!apt-get update -y\n\n# Install dependencies\n!apt-get install -y vsearch kraken2 fastp python3-pip build-essential g++ cmake\n\n# Install Bracken (download from GitHub) - Optional, but keeping for completeness\nbracken_version = \"2.9\"\nbracken_dir = f\"/tmp/Bracken-{bracken_version}\"\nbracken_install_dir = \"/usr/local/bracken\"\n\n# Clean up any existing Bracken installation\n!rm -rf {bracken_dir} {bracken_install_dir} /tmp/bracken.tar.gz\n\n# Download and extract Bracken\n!wget https://github.com/jenniferlu717/Bracken/archive/refs/tags/v{bracken_version}.tar.gz -O /tmp/bracken.tar.gz\n!tar -xzf /tmp/bracken.tar.gz -C /tmp\n\n# Compile kmer2read_distr\n!cd {bracken_dir}/src && make\n\n# Create installation directory and copy files\n!mkdir -p {bracken_install_dir}\n!cp {bracken_dir}/bracken {bracken_install_dir}/\n!cp {bracken_dir}/bracken-build {bracken_install_dir}/\n!cp {bracken_dir}/src/est_abundance.py {bracken_install_dir}/\n!if [ -f {bracken_dir}/src/kmer2read_distr ]; then cp {bracken_dir}/src/kmer2read_distr {bracken_install_dir}/; fi\n!chmod +x {bracken_install_dir}/bracken {bracken_install_dir}/bracken-build {bracken_install_dir}/est_abundance.py\n!if [ -f {bracken_install_dir}/kmer2read_distr ]; then chmod +x {bracken_install_dir}/kmer2read_distr; fi\n\n# Verify Bracken installation\nrequired_bracken_files = [\"bracken\", \"bracken-build\", \"est_abundance.py\"]\nfor f in required_bracken_files:\n    if not os.path.exists(os.path.join(bracken_install_dir, f)):\n        raise FileNotFoundError(f\"Bracken file {f} not found at {bracken_install_dir}/{f}\")\n\n# Install required Python dependencies for Bracken\n!pip install numpy\n\n# Confirm installs\n!which vsearch\n!vsearch --version\n!which kraken2\n!kraken2 --version\n!which fastp\n!fastp --version\n!{bracken_install_dir}/bracken -v\n\n# Setup Kraken2 DB\ndb_dir = \"/kaggle/tmp/kraken2_db\"\nos.makedirs(db_dir, exist_ok=True)\n\n# Download Standard 8GB Kraken2 DB\ndb_tar = f\"{db_dir}/k2_standard_08gb_20240605.tar.gz\"\nif not os.path.exists(f\"{db_dir}/hash.k2d\"):\n    !wget -c https://genome-idx.s3.amazonaws.com/kraken/k2_standard_08gb_20240605.tar.gz -O {db_tar}\n    !tar -xvzf {db_tar} -C {db_dir}\n\n# Set the DB path\ndb_path = db_dir\n\n# Verify DB files\nrequired_files = [\"hash.k2d\", \"taxo.k2d\", \"opts.k2d\", \"database150mers.kmer_distrib\"]\nif not all(os.path.exists(os.path.join(db_path, f)) for f in required_files):\n    raise FileNotFoundError(f\"Required DB files {required_files} not found in {db_path}\")\n\n# Set environment variable for Kraken2\nos.environ[\"KRAKEN2_DB_PATH\"] = db_path","metadata":{"_uuid":"26c7335e-d19a-4145-ac7f-4620d21aa398","_cell_guid":"ffb69463-2e0b-465e-8961-4874d4a6ecdc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-29T13:38:51.778165Z","iopub.execute_input":"2025-09-29T13:38:51.778530Z","iopub.status.idle":"2025-09-29T13:43:49.043081Z","shell.execute_reply.started":"2025-09-29T13:38:51.778501Z","shell.execute_reply":"2025-09-29T13:43:49.042179Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================= Kaggle Setup for vsearch + Kraken2 =================\nimport os\nimport subprocess\nimport glob\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nimport logging\nimport gzip\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\nfh = logging.FileHandler('/kaggle/working/process_log.txt')\nfh.setLevel(logging.INFO)\nlogger.addHandler(fh)\n\ndef run_shell(cmd, check=True):\n    \"\"\"Run a shell command and return (exit_code, stdout, stderr).\"\"\"\n    proc = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    if check and proc.returncode != 0:\n        raise RuntimeError(f\"Command failed: {cmd}\\nstdout: {proc.stdout}\\nstderr: {proc.stderr}\")\n    return proc.returncode, proc.stdout, proc.stderr\n\ndef get_read_length(fastq_file):\n    \"\"\"Estimate read length from FASTQ or FASTQ.gz file.\"\"\"\n    try:\n        if fastq_file.endswith('.gz'):\n            with gzip.open(fastq_file, 'rt') as f:\n                f.readline()  # header\n                seq = f.readline().strip()\n        else:\n            with open(fastq_file, 'r') as f:\n                f.readline()\n                seq = f.readline().strip()\n        return len(seq)\n    except:\n        return 150  # fallback\n\ndef process_sample(sample_id, fastqs, db_path, output_csv, write_header=False):\n    \"\"\"Process a single sample with Kraken2 and write result directly to CSV.\"\"\"\n    report_path = f\"/tmp/{sample_id}.report\"\n    kraken_out = f\"/tmp/{sample_id}.kraken\"\n    try:\n        # Input checks\n        for fq in fastqs:\n            if not os.path.exists(fq):\n                raise FileNotFoundError(f\"Input FASTQ file {fq} not found\")\n            if os.path.getsize(fq) == 0:\n                raise ValueError(f\"Input FASTQ file {fq} is empty\")\n\n        # Kraken2 command\n        paired_flag = \"--paired\" if len(fastqs) == 2 else \"\"\n        compressed_flag = \"--gzip-compressed\" if any(fq.endswith('.gz') for fq in fastqs) else \"\"\n        input_files = \" \".join(fastqs)\n\n        cmd_kraken = (\n            f\"kraken2 --db {db_path} {paired_flag} {compressed_flag} {input_files} \"\n            f\"--output {kraken_out} --report {report_path} \"\n            f\"--threads 2 --use-names --report-zero-counts --memory-mapping\"\n        )\n        run_shell(cmd_kraken)\n\n        # Parse report\n        if not os.path.exists(report_path) or os.path.getsize(report_path) == 0:\n            raise RuntimeError(f\"No valid report for {sample_id}\")\n\n        df = pd.read_csv(\n            report_path, sep=\"\\t\", header=None,\n            names=[\"percent\", \"reads_rooted\", \"reads_direct\", \"rank\", \"taxid\", \"name\"],\n            dtype={\"name\": str}\n        )\n        df2 = df[df[\"rank\"] == \"S\"][[\"name\", \"reads_rooted\"]].copy()\n        df2[\"name\"] = df2[\"name\"].str.strip()\n        df_wide = df2.set_index(\"name\")[\"reads_rooted\"].to_frame().T\n        df_wide[\"SampleID\"] = sample_id\n\n        # Append to CSV\n        df_wide.to_csv(output_csv, mode=\"a\", index=False, header=write_header)\n\n        return True\n\n    except Exception as e:\n        logger.error(f\"Error processing {sample_id}: {e}\")\n        return False\n    finally:\n        for f in [report_path, kraken_out]:\n            if os.path.exists(f):\n                os.remove(f)","metadata":{"_uuid":"20ab2d89-396f-48dc-a977-2b6d42fd4122","_cell_guid":"1fda964b-fc21-4994-acc7-84a4eca709fa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-29T13:44:01.464788Z","iopub.execute_input":"2025-09-29T13:44:01.465110Z","iopub.status.idle":"2025-09-29T13:44:01.479536Z","shell.execute_reply.started":"2025-09-29T13:44:01.465081Z","shell.execute_reply":"2025-09-29T13:44:01.478842Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ——————————————————————————————\n# Step 0: Setup\n# ——————————————————————————————\nworking_dir = \"/kaggle/working\"\nos.makedirs(working_dir, exist_ok=True)\noutput_csv = os.path.join(working_dir, \"microbes_abundances.csv\")\n\ndb_path = os.environ.get(\"KRAKEN2_DB_PATH\", \"/kaggle/tmp/kraken2_db\")\nif not os.path.exists(os.path.join(db_path, \"hash.k2d\")):\n    raise FileNotFoundError(f\"Kraken2 DB not found at {db_path}\")\n\n# ——————————————————————————————\n# Step 1: Find FASTQ samples\n# ——————————————————————————————\nr1_files = glob.glob(\"/kaggle/input/**/*_R1*.fastq*\", recursive=True)\nsamples = defaultdict(list)\nfor r1 in r1_files:\n    sample_id = os.path.basename(r1).split(\"_R1\")[0]\n    r2 = r1.replace(\"_R1\", \"_R2\")\n    samples[sample_id] = [r1, r2] if os.path.exists(r2) else [r1]\n\nother_fastq = set(glob.glob(\"/kaggle/input/**/*.fastq*\", recursive=True)) - set(sum(samples.values(), []))\nfor fq in other_fastq:\n    sample_id = os.path.basename(fq).split(\".\")[0]\n    samples[sample_id] = [fq]\n\nlogger.info(f\"Found {len(samples)} samples to classify.\")\n\n# ——————————————————————————————\n# Step 2: Process sequentially (low memory)\n# ——————————————————————————————\nfirst = True\nsuccess_count = 0\n\nfor sample_id, fastqs in tqdm(samples.items(), desc=\"Classifying samples\"):\n    ok = process_sample(sample_id, fastqs, db_path, output_csv, write_header=first)\n    if ok:\n        success_count += 1\n        first = False  # only write header once\n\nlogger.info(f\"Finished classification. {success_count}/{len(samples)} samples processed successfully.\")\nlogger.info(f\"Results saved to: {output_csv}\")","metadata":{"_uuid":"582a2f6c-4bfe-4f67-972d-0778948376ee","_cell_guid":"3e4edbaa-bfad-48aa-ba16-dfc8395c1316","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-09-29T13:44:08.340148Z","iopub.execute_input":"2025-09-29T13:44:08.340494Z","iopub.status.idle":"2025-09-29T15:32:46.033460Z","shell.execute_reply.started":"2025-09-29T13:44:08.340466Z","shell.execute_reply":"2025-09-29T15:32:46.031681Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}